Token-To-Rules.

Se puede utilizar una arquitectura en la que se clasifique/relacione los prompts del usuario con las reglas existentes.
Para ello, se pueden utilizar tecnologías como "sentence-transformer" para realizar los modelos embeddings (vectores numéricos) 
de los prompts y reglas que tengo. A partir de ahí, se peude utilizar una base de datos vectorial (FAISS) para realizar una búsqueda semántica
que sea capaz de indicar como de "símiles" son el prompt de usuario y cada una de las reglas. Esta similitud se puede hacer mediante la fórmula 
euclídea, cosine, etc. (https://www.baeldung.com/cs/euclidean-distance-vs-cosine-similarity).

El principal problema que veo con este método es que puede haber un prompt que quiera algo pero se asocie una regla que realiza justo lo contrario, 
debido a su similitud. Además, la similutd semántica genérica puede fallar cuando hay reglas antagónicas (ej: free > 50 vs free < 50). Por ejemplo:

Caso 3: Identify servers with CPU idle time less than 1% for 5 minutes
- Bare Metal CPU Underutilized (Idle > 90%) (Score: 0.58)
- Virtual Machine CPU Underutilized (Idle > 90%) (Score: 0.55)
- Bare Metal CPU Overloaded (Idle ≤1% for 1min) (Score: 0.39)
- Bare Metal Memory High Availability (Free > 50%) (Score: 0.39)
- Bare Metal Swap Usage Active (Swap Used > 0) (Score: 0.37)
- Bare Metal Memory Exhausted (Free < 50) (Score: 0.37)
- Bare Metal CPU Critical Overload (Idle ≤1% for 5min) (Score: 0.37)
- Bare Metal Critical Memory Exhaustion (Free <50 for 1min) (Score: 0.36)
- Virtual Machine CPU Overloaded (Idle ≤1% for 1min) (Score: 0.36)
- Virtual Machine CPU Critical Overload (Idle ≤1% for 5min) (Score: 0.34)
- Virtual Machine Memory Exhausted (Free < 50) (Score: 0.22)
- Virtual Machine Critical Memory Exhaustion (Free <50 for 1min) (Score: 0.21)
- Bare Metal Hugepages Depleted (Pages Free = 0) (Score: 0.19)
- Bare Metal Temperature Limit Reached (Temp ≥ Max) (Score: 0.14)
- Bare Metal Critical Temperature Alert (Temp ≥ Critical) (Score: 0.14)
- Bare Metal Critical Fan Speed (Speed < 100 RPM) (Score: 0.14)
- Bare Metal Zombie Processes Active (Count > 0) (Score: 0.13)
- Virtual Machine SSH Access Down (SSH == 0) (Score: 0.13)
- Bare Metal Network Critical: Default Gateway ARP Missing (State=UP & GW Not in ARP) (Score: 0.12)
- Bare Metal Network Non-Standard MTU (MTU ≠1500 on Ethernet) (Score: 0.12)


Para resolver este problema, se puede llevar a cabo "fine-tuning de un modelo de lenguaje con aprendizaje supervisado", de forma que se 
entrena al modelo para que aprenda a asociar pares (promt, regla) con etiquetas de relevancia.



Spacy:

Tutorial crear nuevas fases del pipeline:

In this exercise, you’ll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. A PhraseMatcher with the animal patterns has already been created as the variable matcher.

- Define the custom component and apply the matcher to the doc.
- Create a Span for each match, assign the label ID for "ANIMAL" and overwrite the doc.ents with the new spans.
- Add the new component to the pipeline after the "ner" component.
- Process the text and print the entity text and entity label for the entities in doc.ents.

import spacy
from spacy.language import Language
from spacy.matcher import PhraseMatcher
from spacy.tokens import Span

nlp = spacy.load("en_core_web_sm")
animals = ["Golden Retriever", "cat", "turtle", "Rattus norvegicus"]
animal_patterns = list(nlp.pipe(animals))
print("animal_patterns:", animal_patterns)
matcher = PhraseMatcher(nlp.vocab)
matcher.add("ANIMAL", animal_patterns)

# Define the custom component
@Language.component("animal_component")
def animal_component_function(doc):
    # Apply the matcher to the doc
    matches = matcher(doc)
    # Create a Span for each match and assign the label "ANIMAL"
    spans = [Span(doc, start, end, label="ANIMAL") for match_id, start, end in matches]
    # Overwrite the doc.ents with the matched spans
    doc.ents = spans
    return doc


# Add the component to the pipeline after the "ner" component
nlp.add_pipe("animal_component", after="ner")
print(nlp.pipe_names)

# Process the text and print the text and label for the doc.ents
doc = nlp("I have a cat and a Golden Retriever")
print([(ent.text, ent.label) for ent in doc.ents])





EXTENSIONS + COMPONENTS

Extension attributes are especially powerful if they’re combined with custom pipeline components. In this exercise, you’ll write a pipeline component that finds country names and a custom extension attribute that returns a country’s capital, if available.

A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable CAPITALS.

- Complete the countries_component_function and create a Span with the label "GPE" (geopolitical entity) for all matches.
- Add the component to the pipeline.
- Register the Span extension attribute "capital" with the getter get_capital.
- Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents.

import json
import spacy
from spacy.language import Language
from spacy.tokens import Span
from spacy.matcher import PhraseMatcher

with open("exercises/en/countries.json", encoding="utf8") as f:
    COUNTRIES = json.loads(f.read())

with open("exercises/en/capitals.json", encoding="utf8") as f:
    CAPITALS = json.loads(f.read())

nlp = spacy.blank("en")
matcher = PhraseMatcher(nlp.vocab)
matcher.add("COUNTRY", list(nlp.pipe(COUNTRIES)))


@Language.component("countries_component")
def countries_component_function(doc):
    # Create an entity Span with the label "GPE" for all matches
    matches = matcher(doc)
    doc.ents = [Span(doc, start, end, label="GPE") for match_id, start, end in matches]
    return doc


# Add the component to the pipeline
nlp.add_pipe("countries_component")
print(nlp.pipe_names)

# Getter that looks up the span text in the dictionary of country capitals
get_capital = lambda span: CAPITALS.get(span.text)

# Register the Span extension attribute "capital" with the getter get_capital
Span.set_extension("capital", getter=get_capital)

# Process the text and print the entity text, label and capital attributes
doc = nlp("Czech Republic may help Slovakia protect its airspace")
print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])




LOT OF TEXTS -> FASTER THIS WAY

BAD:

docs = [nlp(text) for text in LOTS_OF_TEXTS]

GOOD:

docs = list(nlp.pipe(LOTS_OF_TEXTS))


nlp.pipe también soporta pasar tuplas de texto/contexto si as_tuples = True. Útil
para pasar metadatos adicionales.

from spacy.tokens import Doc

Doc.set_extension("id", default=None)
Doc.set_extension("page_number", default=None)

data = [
    ("This is a text", {"id": 1, "page_number": 15}),
    ("And another text", {"id": 2, "page_number": 16}),
]

for doc, context in nlp.pipe(data, as_tuples=True):
    doc._.id = context["id"]
    doc._.page_number = context["page_number"]




ENTRENAR MODELOS:

Es mejor entrenar el modelo para que reconozca ENTITYs simples como LOCATION. Para saber si esa 
LOCATION se trata de un TOURIST_DESTINATION en ese contexto, se puede utilizar un componente basado en reglas.

Si intentamos que el modelo reconoza directamente TOURIST_DESTINATION, posiblemente no funcione y no se pueda 
entrenar bien. Pues el modelo necesita entrenarse con ENTITYs mas genericas.



En el TFG puedo poner la opción de Sentence-Trasnformer como experimentación e indicar detalles como que no se consiguen resultados adecuados
(mirar reunión). Puedo poner también el por qué usar SpaCy y la ayuda que tiene el poder entrenarlo. Explicar que necesito más datos como reglas
y prompts.

Para generar reglas, mirar artículo que me ha pasado Javier. Si no sirve, intentar buscar en deepseek/chatgpt.

Mirar pluggins de spacy en caso de que sean de ayuda (mirar trello).

I want to deploy a web server that has minimum 6 CPUs available, with no transmision error neither receive errors from network. Moreover, I don't want the server to have high temperatures and I must be able to access to it with SSH.


CONSIDERACIÓN SOBRE SINONIMOS

Si implemento los sinonimos con un diccionario como el siguiente:

{
    "compute": ["processing", "capacity", "throughput", "calculation"],
    "network": ["bandwidth", "latency", "throughput", "transmission", "connectivity"],
    "memory": ["ram", "swap", "buffers", "allocation", "storage"],
    "speed": ["latency", "throughput", "performance", "response"],
    "critical": ["emergency", "severe", "alert", "danger"],
    "overload": ["saturation", "maxout", "overutilization"]
}

Tenemos un problema, si aparece el token "processing" no detectará que es sinónimo de "compute", "capacity"... Para solucionar esto hay 2 formas:

1: Cuando se procesen los token, recorrer las listas y comprobar si se encuentra la palabra en algún conjunto. Sin embargo, es muy ineficiente -> Por cada búsqueda el tiempo es lineal O(n).
2: Crear un diccionario invertido en el que cada palabra se encuentre relacionada con sus sinónimos. Por ejemplo:
        INVERTED_TECHNICAL_SYNONYMS = {
            "cpu": ["cpu", "processor", "core"],
            "processor": ["cpu", "processor", "core"],
            "core": ["cpu", "processor", "core"],
            "network": ["network", "lan", "ethernet"],
            "lan": ["network", "lan", "ethernet"],
            "ethernet": ["network", "lan", "ethernet"],
            "memory": ["memory", "ram", "storage"],
            "ram": ["memory", "ram", "storage"],
            "storage": ["memory", "ram", "storage"]
        }
    Esto permite 2 cosas: Que cada palabra esté relacionada con su sinónimo y, que cada búsqueda sea constante O(1), pues sería comprobar solamente si el token se encuentra entre las keys del diccionario
    mediante hash tables. 
    La conversión de un diccionario normal a invertido tiene una complejidad de O(n), pero esto se realiza solo 1 vez y no por cada token procesado.

He decidido utilizar la segunda opción por razones obvias. Sin embargo, en el fichero "technical_synonyms" se utiliza un diccionario normal para que sea legible y de fácil actualización.
